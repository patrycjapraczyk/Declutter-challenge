{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## A simple model based on logistic regression for determining if the comments are useful or not.\n",
    "\n",
    "Import libraries and parse CSV data --> include only the comment strings and their labels (non-information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "data = pd.read_csv(\"./../data/train_set_0520.csv\", usecols=['type', 'comment', 'non-information'])\n",
    "\n",
    "#.apply(str).apply()\n",
    "values = data['non-information'].values\n",
    "values = np.where(values == 'yes', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess data:\n",
    "1. remove all special characters\n",
    "2. TODO: expand contradictions (don't = do not etc)\n",
    "3. remove special characters\n",
    "4. stemming --> put the word into its most basic form\n",
    "5. lemmatisation --> removes the word's affixes to get to the basic form of the word\n",
    "\n",
    "Observations: Removing stopwords decreased accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0    18\n1    40\n2    69\n3    51\n4     9\nName: comment, dtype: int64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helpers.data_preprocessing import DataProcesser\n",
    "dp = DataProcesser()\n",
    "\n",
    "data['comment'] = data['comment'].apply(str)\n",
    "data['comment'] = data['comment'].apply(dp.preprocess)\n",
    "length_data = data['comment'].apply(lambda c: len(c))\n",
    "comments = data['comment']\n",
    "length_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the comments into train (used for training the model) and test data (used for evaluating the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "431                                  open a share databas\n213     execut a callabl task that provid a valu after...\n730     copi the select entri and mat them with the se...\n1005                               auto gener method stub\n130     keep track of chang made to the column like re...\n                              ...                        \n769                           test the field is legal set\n350     initi the compon the layout the data structur ...\n1275    panel getunmanag addedit unablemovegroup group...\n71      to remov old en or add it to a list of entri t...\n599     need to toggl a twice to make sure everyth is ...\nName: comment, Length: 983, dtype: object"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "comments_train, comments_test, y_train, y_test = train_test_split(comments, values, test_size=0.25, random_state=1000)\n",
    "comments_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorise data - map a numerical value to each word\n",
    "\n",
    "This is based on the Bag-of-Words Model. It ignores the order of words and focuses only on their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<328x921 sparse matrix of type '<class 'numpy.int64'>'\n\twith 3403 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO add length after count vectoriser --> seperate into files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(comments_train)\n",
    "\n",
    "\n",
    "x_train = vectorizer.transform(comments_train)\n",
    "x_test  = vectorizer.transform(comments_test)\n",
    "\n",
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a logistic regression model and fit the training data into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression()"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.fixes import loguniform\n",
    "#param_grid = {'C': loguniform(1e0, 1e3)}\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Evaluate the model based on the following properties:\n",
    "* <b>accuracy</b> - How often a data point is classified correctly?\n",
    "The number of true positives and true negatives divided by the number of true positives, true negatives, false positives, and false negatives\n",
    "* <b>precision</b> - What proportion of positive identifications was actually correct?\n",
    "The number of true positives divided by the number of true positives and false positives\n",
    "* <b>recall</b> - What proportion of actual positives was identified correctly?\n",
    "The number of true positives divided by the number of true positives and false negatives\n",
    "* <b>F1 score</b>- The F1 Score is the 2*((precision*recall)/(precision+recall)).\n",
    " conveys the balance between the precision and the recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 921 features per sample; expecting 1579",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-12-ac60a43d607d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel_selection\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcross_val_predict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcross_validate\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0my_pred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mclassifier\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_test\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m#lassifier.predict(x_test)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# Model Evaluation metrics\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py\u001B[0m in \u001B[0;36mpredict\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    305\u001B[0m             \u001B[0mPredicted\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mlabel\u001B[0m \u001B[0mper\u001B[0m \u001B[0msample\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    306\u001B[0m         \"\"\"\n\u001B[0;32m--> 307\u001B[0;31m         \u001B[0mscores\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdecision_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    308\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mscores\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    309\u001B[0m             \u001B[0mindices\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mscores\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py\u001B[0m in \u001B[0;36mdecision_function\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    284\u001B[0m         \u001B[0mn_features\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcoef_\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    285\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mn_features\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 286\u001B[0;31m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001B[0m\u001B[1;32m    287\u001B[0m                              % (X.shape[1], n_features))\n\u001B[1;32m    288\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: X has 921 features per sample; expecting 1579"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "\n",
    "y_pred = classifier.predict(x_test)#lassifier.predict(x_test)\n",
    "\n",
    "# Model Evaluation metrics\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "print('Accuracy Score : ' + str(accuracy_score(y_test,y_pred)))\n",
    "print('Precision Score : ' + str(precision_score(y_test,y_pred)))\n",
    "print('Recall Score : ' + str(recall_score(y_test,y_pred)))\n",
    "print('F1 Score : ' + str(f1_score(y_test,y_pred)))\n",
    "\n",
    "# Attempted to optimise hyperparameters - didn't work\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# clf = LogisticRegression()\n",
    "# grid_values = {'C':[0.001,.009,0.01,.09,1,5,10,25]}\n",
    "# grid_clf_acc = GridSearchCV(clf, param_grid = grid_values,scoring = 'recall')\n",
    "# grid_clf_acc.fit(x_train, y_train)\n",
    "#\n",
    "# #Predict values based on new parameters\n",
    "# y_pred_acc = grid_clf_acc.predict(x_test)\n",
    "#\n",
    "# # New Model Evaluation metrics\n",
    "# print('Accuracy Score : ' + str(accuracy_score(y_test,y_pred_acc)))\n",
    "# print('Precision Score : ' + str(precision_score(y_test,y_pred_acc)))\n",
    "# print('Recall Score : ' + str(recall_score(y_test,y_pred_acc)))\n",
    "# print('F1 Score : ' + str(f1_score(y_test,y_pred_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.7042682926829268\n",
      "Precision Score : 0.0\n",
      "Recall Score : 0.0\n",
      "F1 Score : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrycja/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "randomForestClassifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "randomForestClassifier.fit(x_train, y_train)\n",
    "\n",
    "y_pred = randomForestClassifier.predict(x_test)\n",
    "\n",
    "# Model Evaluation metrics\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "print('Accuracy Score : ' + str(accuracy_score(y_test,y_pred)))\n",
    "print('Precision Score : ' + str(precision_score(y_test,y_pred)))\n",
    "print('Recall Score : ' + str(recall_score(y_test,y_pred)))\n",
    "print('F1 Score : ' + str(f1_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}